# SPEED-Intelligent-Racing-Agents

This repository documents the design, implementation, ablation, and rigorous benchmarking of two distinct AI approaches for autonomous racing in Unity: a deterministic waypoint-based heuristic controller and a deep reinforcement learning (DRL) agent built with Unity ML-Agents (PPO). The work provides an academically robust framework for comparing classical and modern approaches‚Äîemphasizing reproducibility, open science, and in-depth technical documentation for MSc-level research.

## Playtest Prototype

Try the interactive playtest prototype on Itch.io:

üèÅ **[Play SPEED Racing Agents Demo]([https://your-username.itch.io/speed-racing-agents](https://sujitvarma.itch.io/ml-agent-playtest-prototype))**

Experience the Enhanced Natural DRL agent in action, compare its behavior with your gameplay, and see the research findings firsthand.

## Agent Demonstration Videos

### Side-by-Side Agent Comparison

<div align="center">

| Heuristic Agent | Natural DRL Agent |
|:---------------:|:-----------------:|
| *Video placeholder - Deterministic waypoint-based controller* | *Video placeholder - PPO-trained adaptive agent* |
| ![Heuristic Agent Demo](https://via.placeholder.com/400x225/1e3a8a/ffffff?text=Heuristic+Agent+Video) | ![DRL Agent Demo](https://via.placeholder.com/400x225/dc2626/ffffff?text=DRL+Agent+Video) |
| **Characteristics:** | **Characteristics:** |
| ‚Ä¢ Precise waypoint following | ‚Ä¢ Adaptive behavior learning |
| ‚Ä¢ Deterministic responses | ‚Ä¢ Human-like smoothness |
| ‚Ä¢ Collision-free navigation | ‚Ä¢ Dynamic decision making |

</div>

> **Note:** Video demonstrations will showcase real-time performance comparisons, lap time differences, and behavioral analysis between the two approaches.

## Project Motivation

Autonomous racing is a prime testbed for intelligent agent research. It requires tight integration of perception, sequential policy, real-time optimization, and adaptability to diverse environmental conditions. This project addresses critical open questions:

‚Ä¢ When and where do engineered heuristic controllers outperform or underperform RL agents?
‚Ä¢ How does reward shaping and neural architecture affect RL agent learning, stability, and generalization?
‚Ä¢ Can DRL agents demonstrate truly human-like behavior, as measured by both efficiency and naturalness?

## Summary of Contributions

‚Ä¢ **Heuristic Controller**: Fully deterministic, high-performance waypoint follower. Adaptively controls speed and heading for efficient, collision-free lap completion. All parameters and logs are documented for reproducibility.
‚Ä¢ **DRL Racing Agent**: PPO-based agent using raycast-derived sensor input, curriculum learning, and custom reward structure. Multiple ablation studies highlight the trade-offs in architectural depth, reward complexity, and agent robustness.
‚Ä¢ **Human Benchmark Suite**: Tracks and scenes support human demos for direct comparison‚Äîlap times, steering smoothness, and behavioral metrics.
‚Ä¢ **Open Experimental Pipeline**: Source code, configuration files, trained models, raw logs, and exhaustive documentation are provided to enable full reproduction of all reported findings.

## Repository Structure

```
SPEED-Intelligent-Racing-Agents/
‚îú‚îÄ‚îÄ Heuristic-Agent-Project/     # Deterministic controller (Unity 2021.3.45f1)
‚îÇ   ‚îú‚îÄ‚îÄ Assets/
‚îÇ   ‚îú‚îÄ‚îÄ ProjectSettings/
‚îÇ   ‚îî‚îÄ‚îÄ Packages/
‚îú‚îÄ‚îÄ DRL-Agent-Project/           # PPO agent, custom reward, evaluation scenes
‚îÇ   ‚îú‚îÄ‚îÄ Assets/
‚îÇ   ‚îú‚îÄ‚îÄ ProjectSettings/
‚îÇ   ‚îú‚îÄ‚îÄ Packages/
‚îÇ   ‚îî‚îÄ‚îÄ Trained-Models/
‚îú‚îÄ‚îÄ Results-and-Analysis/        # Performance logs, comparison data, CSVs
‚îú‚îÄ‚îÄ Documentation/               # Research paper, user/setup guides
‚îÇ   ‚îú‚îÄ‚îÄ SPEED-Dissertation.pdf
‚îÇ   ‚îî‚îÄ‚îÄ Setup-Instructions.md
‚îî‚îÄ‚îÄ README.md
```

## Technical Stack

‚Ä¢ **Game Engine**: Unity 2021.3.45f1 LTS
‚Ä¢ **ML Toolkit**: Unity ML-Agents 0.30.0 (PPO algorithm)
‚Ä¢ **Programming**: C# (Unity scripting), Python 3.8+
‚Ä¢ **Hardware**: 8GB+ RAM, CUDA GPU recommended for DRL
‚Ä¢ **Visualization**: Unity scene GUI, TensorBoard, custom plots
‚Ä¢ **Versioning**: Git, atomic commits for all experiments

## Setup and Reproduction

### Prerequisites

‚Ä¢ Unity 2021.3.45f1 LTS (mandatory version)
‚Ä¢ Python 3.8+ with pip
‚Ä¢ ML-Agents 0.30.0 (`pip install mlagents==0.30.0`)

### Installation Steps

1. **Clone the repository:**
```bash
git clone https://github.com/Sujyeet/SPEED-Intelligent-Racing-Agents.git
cd SPEED-Intelligent-Racing-Agents
```

2. **Open projects in Unity Hub:** select either `Heuristic-Agent-Project` or `DRL-Agent-Project`.
‚Ä¢ Only the folders `Assets`, `Packages`, and `ProjectSettings` are strictly required.
‚Ä¢ On first open, import will take several minutes due to dependency resolution.

3. **(Optional) Set up a Python environment for training:**
```bash
python -m venv racing_env
# Windows: racing_env\Scripts\activate
# Mac/Linux: source racing_env/bin/activate
pip install -r requirements.txt
```

## Running and Evaluation

### Heuristic Agent

‚Ä¢ Load `Heuristic-Agent-Project` in Unity
‚Ä¢ Select a race scene (e.g., `Assets/Karting/Scenes/MainScene.unity`)
‚Ä¢ Press Play to run the agent; logs are saved for every episode (Paste the desired output directory in the editor)

### DRL Agent (PPO)

‚Ä¢ Open `DRL-Agent-Project` in Unity, ensure ML-Agents is present
‚Ä¢ Assign any `.onnx` model from `Trained-Models/` to the kart agent in test scenes
‚Ä¢ Set Behavior Type to "Inference Only" and press Play to visualize evaluation
‚Ä¢ **Retraining**: See `Documentation/Setup-Instructions.md` for hyperparameters, scripts, and training protocol

### Human Benchmark

‚Ä¢ Scenes allowing manual play (keyboard/controller) are included, with instructions in the UI
‚Ä¢ Run with identical evaluation pipeline to ensure compatibility of all recorded metrics

## Methodology and Experiments

### Controlled Testing

‚Ä¢ Evaluation on fixed-seed, multi-lap racing tracks for statistical robustness
‚Ä¢ Lap time, completion rate, collisions, steering profiles, and path deviation are recorded per episode

### Ablation Studies

‚Ä¢ **Reward structure**: From complex (multiple terms) to minimal (progress, smoothness, speed)
‚Ä¢ **Network architecture**: 3-layer/256 vs. 2-layer/128 PPO; training stability and convergence tracked
‚Ä¢ **Simulation time scale & curriculum**: Real-time vs 10x acceleration; effect on sample efficiency
‚Ä¢ **Generalization**: Evaluations on alternate tracks for out-of-distribution robustness
‚Ä¢ **Result**: Simpler reward and lightweight networks consistently improved DRL reliability. See main paper for summary tables (e.g., Table IV, Table V) and cumulative reward learning curves.

### Performance Metrics (example table)

| Agent | Mean Lap Time (s) | Std Dev | Collision Rate | Human-Likeness |
|-------|------------------|---------|----------------|-----------------|
| Heuristic Agent | 41.52 | 0.09 | 0% | Low |
| PPO DRL Agent (Baseline) | 39.8 | 1.4 | <2% | High |
| DRL Agent (Enhanced Humanlike) | 43.0 | 1.1 | <1% | Very High |
| Human Player | 44.2 | 2.8 | ~3% | Baseline |

### Behavioral Analysis

Beyond speed, the work quantifies "natural" behavior:
‚Ä¢ **Path deviation and steering variance**‚Äîplotted time series show DRL agents closely track human smoothness, unlike the mechanical, deterministic baseline
‚Ä¢ **Error and recovery**: Analysis of wall-collisions and recovery strategies
‚Ä¢ **Comprehensive logging**: All logs available for reproduction in Results-and-Analysis/

### Key Lessons

‚Ä¢ Over-complexity (deep networks, multi-term rewards) led to instability and slow convergence
‚Ä¢ Well-tuned heuristics provide reliability but limited adaptability to novel track layouts
‚Ä¢ Simplified DRL with targeted reward shaping yields both superior lap times and higher human-likeness scores in controlled experiments

## Full Reproducibility Commitment

‚Ä¢ All experiments use documented seeds and config files; every result in the paper can be regenerated from scripts in this repository
‚Ä¢ **Paper draft**: Documentation/SPEED-Dissertation.pdf (complete analyses, figures, and extended results)
‚Ä¢ **Installation and reproducibility guide**: Documentation/Setup-Instructions.md (step-by-step for every OS)

## Collaboration & Academic Use

‚Ä¢ **Academic collaboration**: Designed for MSc and PhD research use; contact author for dataset sharing, integration studies, or advanced experiments
‚Ä¢ **Contributing**: Issue tracker and pull requests are welcome for reproducibility, cross-validation, or improved agent design studies

## Citation

If this work, code, or methodology contributes to your research, please cite:

```bibtex
@mastersthesis{sujyeet2025speed,
  title={SPEED: Intelligent Racing Agents Using Deep Reinforcement Learning and Unity ML-Agents},
  author={Sujyeet, [Your Full Name]},
  year={2025},
  school={[Your University]},
  type={MSc Thesis}
}
```

## Author & Contact

‚Ä¢ Sujyeet (MSc Autonomous Racing, [your.email@domain])
‚Ä¢ Queen Mary University of London (for academic coordination)
‚Ä¢ For technical questions, open a GitHub Issue or contact by email

## License

All project source, code, models, data, and documentation are released under the MIT License. See LICENSE for details. Intended for research, education, and open collaboration.
